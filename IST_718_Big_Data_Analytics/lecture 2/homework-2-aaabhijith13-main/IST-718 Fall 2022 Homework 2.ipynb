{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFG-FbiBuU3W"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmrn6lD--kF"
   },
   "source": [
    "**Question 0 (-2 pts if not provided):**  Enter your name and SU ID in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bizycRXz_uM6"
   },
   "source": [
    "Your name and SU ID Here:Abhijith Anil Vamadev 495204994\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko1ywZU6uU3c"
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professors: \n",
    "  - Willard Williamson <wewillia@syr.edu>\n",
    "  - Emory Creel <emcreel@syr.edu>\n",
    "- Faculty Assistants: \n",
    "  - Warren Justin Fernandes <warrenfds25@gmail.com>\n",
    "  - Ruchita Hiteshkumar Harsora <\trharsora@g.syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
    "- Google Colab is the official class runtime environment so you should test your code on Colab before submission.\n",
    "- Do not modify cells marked as grading cells or marked as do not modify.\n",
    "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
    "`Runtime `$\\rightarrow$ Factory reset runtime followed by Runtime $\\rightarrow$ Run All.  All runtime errors will result in a minimum penalty of half off.\n",
    "- All plots shall include descriptive title and axis labels.  Plot legends shall be included where possible.  Unless stated otherwise, plots can be made using any Python plotting package.\n",
    "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells.\n",
    "- Don't add or remove files from your git repo.\n",
    "- Do not change file names in your repo.  This also means don't change the title of the ipython notebook.\n",
    "- You are free to add additional code cells around the cells marked `your code here`.\n",
    "- import * is not allowed because it is considered a very bad coding practice and in some cases can result in a significant delay (which slows down the grading process) in loading imports.  For example, the statement `from sympy import *` is not allowed.  You must import the specific packages that you need. \n",
    "- The graders reserve the right to deduct points for subjective things we see with your code.  For example, if we ask you to create a pandas data frame to display values from an investigation and you hard code the values, we will take points off for that.  This is only one of many different things we could find in reviewing your code.  In general, write your code like you are submitting it for a code peer review in industry.  \n",
    "- Level of effort is part of our subjective grading.  For example, in cases where we ask for a more open ended investigation, some students put in significant effort and some students do the minimum possible to meet requirements.  In these cases, we may take points off for students who did not put in much effort as compared to students who put in a lot of effort.  We feel that the students who did a better job deserve a better grade.  We reserve the right to invoke level of effort grading at any time.\n",
    "- Your notebook must run from start to finish without requiring manual input by the graders.  For example, do not mount your personal Google drive in your notebook as this will require graders to perform manual steps.  In short, your notebook should run from start to finish with no runtime errors and no need for graders to perform any manual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48903,
     "status": "ok",
     "timestamp": 1663264999657,
     "user": {
      "displayName": "Willard Williamson",
      "userId": "04507347240949254966"
     },
     "user_tz": 240
    },
    "id": "GWwdob0jeo5a",
    "outputId": "a9681aa5-919b-4883-8c39-afb74fae83a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicator_gapminder_population.csv\n",
      "indicator_gapminder_under5mortality.csv\n",
      "indicator_life_expectancy_at_birth.csv\n",
      "indicator_undata_total_fertility.csv\n",
      "continents.tsv\n",
      "city_temperatures.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Do not change or modify this cell\n",
    "\n",
    "# if pyspark is already installed, will print a message indicating pyspark already installed\n",
    "pip install pyspark &> /dev/null\n",
    "\n",
    "# define an array of data file names\n",
    "data_file_array=(\"indicator_gapminder_population.csv\" \"indicator_gapminder_under5mortality.csv\" \"indicator_life_expectancy_at_birth.csv\" \"indicator_undata_total_fertility.csv\" \"continents.tsv\")\n",
    "\n",
    "# Download the UN Indicator data from github\n",
    "# for each data file\n",
    "for file in ${data_file_array[@]}; do\n",
    "  echo ${file}\n",
    "  # if the data file does not exist on the local computer\n",
    "  if [[ ! -f ./${file} ]]; then \n",
    "    # download the data file from github and save it on the local computer\n",
    "    wget https://raw.githubusercontent.com/wewilli1/ist718_data/master/un_indicator_data/${file} &> /dev/null\n",
    "  fi  \n",
    "done\n",
    "\n",
    "# Download the city temp data file from github\n",
    "data_file=city_temperatures.csv\n",
    "echo ${data_file}\n",
    "\n",
    "# If the data file does not exist in the colab environment\n",
    "if [[ ! -f ./${data_file} ]]; then \n",
    "   # download the data file from github and save it in this colab environment instance\n",
    "   wget https://raw.githubusercontent.com/wewilli1/ist718_data/master/${data_file} &>/dev/null\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ocf0EXJ1uU3l"
   },
   "source": [
    "# Part 1: Map / Reduce\n",
    "Part 1 uses the central limit theorem to provide you with an opportunity to practice using the map / reduce programming paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKpmsrx9uU3k"
   },
   "source": [
    "Note: Starting with this homework and moving forward, students are expected to use spark as opposed to numpy or pandas unless directed otherwise.  This note will not be repeated in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRRer_OAuU3l"
   },
   "source": [
    "# Question 1: Central Limit Theorem Simulation (10 pts)\n",
    "Create a single dimensional pyspark RDD named bernoulli_rdd that contains 10,000 Bernoulli probability distribution data points consisting of integer 0 or 1.  P(0) = P(1) = 0.5.  Use only pyspark RDDs to complete this question.  Create a histogram of the RDD to show that it follows the Bernoulli distribution.  As always, you are allowed to use python / pandas / matplotlib to make your plot but do use spark to create thedistribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "f1hBopNLuU3m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Bernoulli distrubiton of 1 and 0 over 10000 trials of 0.5 probability')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = SparkSession.builder.appName('spark-intro').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as fn\n",
    "n, p = 1, .5  # number of trials, probability of each trial\n",
    "np.random.seed(0)\n",
    "s = np.random.binomial(n, p, 10000)\n",
    "bernoulli_rdd = sc.parallelize(s)\n",
    "plt.hist(bernoulli_rdd.collect())\n",
    "plt.xlabel('Bernoulli distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bernoulli distrubiton of 1 and 0 over 10000 trials of 0.5 probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljbn1UMPuU3m"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ez83g72ruU3m"
   },
   "source": [
    "# Question 2: Sample the Bernoulli distribution using CLT (10 pts)\n",
    "Using principals from CLT theory, create a new 2 dimensional RDD named bernoulli_sample_rdd that contains sample data from bernoulli_rdd.  The length of bernoulli_sample_rdd should be the number of samples.  Each data element in bernoulli_sample_rdd should contain num_obs_per_sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KrMgO7NjuU3n"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "import random\n",
    "bernoulli_sample_rdd = [random.sample(bernoulli_rdd.collect(), 100) for i in range(100) ] #100 samples with each sample containing 100 items\n",
    "bernoulli_sample_rdd = sc.parallelize(bernoulli_sample_rdd)\n",
    "# bernoulli_sample_rdd = [bernoulli_rdd.sample(False, 0.01) for i in range(100)]\n",
    "# # bernoulli_sample_rdd[0].collect()\n",
    "# bernoulli_sample_rdd = sc.parallelize(s)\n",
    "# len(bernoulli_sample_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ9Wc6QRuU3n"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhzs-YMsuU3n"
   },
   "source": [
    "# Question 3: Create Sampling Distribution (20 pts)\n",
    "Using pyspark map and reduce, create a new RDD named bernoulli_sample_mean_rdd that contains the sampling distribution of the means of the samples contained in bernoulli_sample_rdd.  Create a histogram from bernoulli_sample_mean_rdd to prove that the resulting distribution follows the principals of the CLT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8vV3MaiZuU3o"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 104.0 failed 1 times, most recent failure: Lost task 3.0 in stage 104.0 (TID 1667) (AbhijithAV.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ABHIJI~1\\AppData\\Local\\Temp/ipykernel_16564/1082875349.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbernoulli_sample_mean_rdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbernoulli_sample_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbernoulli_sample_mean_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# plt.xlabel('Bernoulli distribution')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# plt.ylabel('Frequency')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m    948\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 104.0 failed 1 times, most recent failure: Lost task 3.0 in stage 104.0 (TID 1667) (AbhijithAV.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def average(x):\n",
    "    return float(sum(x)/len(x))\n",
    "bernoulli_sample_mean_rdd = bernoulli_sample_rdd.map(average)\n",
    "\n",
    "plt.hist(bernoulli_sample_mean_rdd.collect())\n",
    "# plt.xlabel('Bernoulli distribution')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Bernoulli distrubiton of 1 and 0 over 10000 trials of 0.5 probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 108.0 failed 1 times, most recent failure: Lost task 14.0 in stage 108.0 (TID 1742) (AbhijithAV.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ABHIJI~1\\AppData\\Local\\Temp/ipykernel_19540/3791919241.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m#rdd.map(lambda x: (count(x), 2)).collect()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m    948\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 108.0 failed 1 times, most recent failure: Lost task 14.0 in stage 108.0 (TID 1742) (AbhijithAV.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = SparkSession.builder.appName('spark-intro').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as fn\n",
    "d = [[1,2,3],[4,5,6], [7,8,9,9]]\n",
    "rdd = sc.parallelize(d)\n",
    "rdd.collect()\n",
    "rdd.mapValues(lambda x: sum(x)/len(x)).collect()\n",
    "#rdd.map(lambda x: (count(x), 2)).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbhxtrtTuU3p"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGSyg5K_IKnV"
   },
   "source": [
    "# Question 4 Map / Reduce (Total 20 pts): \n",
    "## Question 4 Part 1 (15 points):\n",
    "The code below implements part of the standard deviation formula shown below using map reduce.  The goal is to find the standard deviation of temperature data for Boston in the year 2019.  I provided code which partially implements the formula by computing the average $\\mu$ in the standard deviation formula below. Your job is to finish implementing the formula using ONLY spark map, mapValues, or reduceByKey such that it EXACTLY implements the equation. Start by reading and understanding the partial implementation.  Do not use ANY python built in functions except math.sqrt.  Note that even math.sqrt may only be called inside a map / reduce callback function.  Hard coded constants are not allowed.  For example, you may not assume the constant 364 in the denominator for N - 1, you must calculate N and subtract 1 in the map reduce call back function context.  Do not use ANY spark built in functions except the specific map / reduce functions called out above.  For example, don't use an RDD's count() function, don't use any numpy or python built in functions to calculate standard deviation.  Implement all code in callback functions implemented by you.<br>\n",
    "\n",
    "The way I coded it, the following line of code prints the expected standard deviation using only map, reduceByKey, and mapValues.  See an example of how I implemented the map reduce chain below.  I am expecting students to do something similar though you are free to add or remove map reduce stages as you see fit (you are not limited to 3 stages - use more or less stages as you see fit).  The main thing is to do ALL calculations in the context of spark map reduce call back functions as shown in the line of code below:\n",
    "```\n",
    "print(boston.map().reduceByKey().mapValues())\n",
    "```\n",
    "\n",
    "$STD(X,Y)=\\sqrt{\\frac{\\sum_{n=1}^{N}(x_i-\\mu)^2}{N-1}}$<br>\n",
    "Where:<br>\n",
    "- STD = Sample Standard Deviation\n",
    "- N = Total number of observations in the sample\n",
    "- $\\mu$ is the sample mean\n",
    "- $x_i$ are sample observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13897,
     "status": "ok",
     "timestamp": 1663265179024,
     "user": {
      "displayName": "Willard Williamson",
      "userId": "04507347240949254966"
     },
     "user_tz": 240
    },
    "id": "1t156DaQIqRJ",
    "outputId": "6ebe1a73-7d32-4f30-8f38-f16f15d86944"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 105.0 failed 1 times, most recent failure: Lost task 0.0 in stage 105.0 (TID 1665) (AbhijithAV.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ABHIJI~1\\AppData\\Local\\Temp/ipykernel_20232/3655522341.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# remove the header: Region,Country,State,City,Month,Day,Year,AvgTemperature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcity_temp_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mcity_temp_noheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcity_temp_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m         \"\"\"\n\u001b[1;32m-> 1586\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1231\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 105.0 failed 1 times, most recent failure: Lost task 0.0 in stage 105.0 (TID 1665) (AbhijithAV.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import math\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"city_rdd\") \\\n",
    "    .getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# read the city temp file into an rdd\n",
    "city_temp_rdd = spark.sparkContext.textFile(\"city_temperatures.csv\")\n",
    "\n",
    "# remove the header: Region,Country,State,City,Month,Day,Year,AvgTemperature\n",
    "header = city_temp_rdd.first()\n",
    "city_temp_noheader = city_temp_rdd.filter(lambda x: x != header)\n",
    "\n",
    "# remove the index col\n",
    "city_temp_noindex = city_temp_noheader.map(lambda x: x.split(\",\")[1:])\n",
    "\n",
    "# cast the data types to the correct data types\n",
    "def cast_types(x):\n",
    "  return_val = []\n",
    "  try:\n",
    "    return_val = [x[0], x[1], x[2], x[3], int(x[4]), int(x[5]), int(x[6]), float(x[7])]\n",
    "  except ValueError:\n",
    "    # An exception was raised during numeric type casting, set the \n",
    "    # numeric types to a string 'NAN' to be filtered in the next stage.\n",
    "    return_val = [x[0], x[1], x[2], x[3], 'NAN', 'NAN', 'NAN', 'NAN']\n",
    "\n",
    "  return return_val\n",
    "\n",
    "# cast the types then remove rows which did not successfully cast to numeric types\n",
    "city_temp_cast = city_temp_noindex.map(cast_types)\n",
    "city_temp_filter_nan = city_temp_cast.filter(lambda x: False if \"NAN\" in x else True)\n",
    "\n",
    "# define some useful index variables to make the code more readable\n",
    "CITY_INDEX = 3\n",
    "YEAR_INDEX = 6\n",
    "TEMP_INDEX = 7\n",
    "\n",
    "# Filter by boston 2019\n",
    "# When matching strings its always good to compare against lower or upper case\n",
    "# to avoid mismatches cause by case differences.\n",
    "boston = city_temp_filter_nan.filter(lambda x: True if (x[CITY_INDEX].lower() == \"boston\" and x[YEAR_INDEX] == 2019) else False)\n",
    " \n",
    "# Define some variables to make the code more readable\n",
    "COUNT_INDEX = 0 # the index where ones are located to sum the count (the denominator)\n",
    "SUM_INDEX = 1   # the index where values are located for sums (the numerator)\n",
    "\n",
    "# Compute the average.  City is arbitrarily chosen to be the key for reduceByKey.\n",
    "ave_rdd = boston.map(lambda x: (x[CITY_INDEX], (1, x[TEMP_INDEX]))).\\\n",
    "          reduceByKey(lambda x, y: (x[COUNT_INDEX] + y[COUNT_INDEX], x[SUM_INDEX] + y[SUM_INDEX])).\\\n",
    "          mapValues(lambda x: x[SUM_INDEX] / x[COUNT_INDEX])\n",
    "\n",
    "# Save the average in a python variable to be used for calculating standard deviation\n",
    "ave = ave_rdd.collect()[0][1]\n",
    "print(ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBbPxzc3IslK"
   },
   "outputs": [],
   "source": [
    "# Your standard deviation map reduce code here.  \n",
    "# Use the \"ave\" variable above in your calculations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFL3N3VVqPpU"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DESFaykl49BU"
   },
   "source": [
    "## Question 4 Part 2: (5 Points):\n",
    "In the partial implementation code above, I use reduceByKey as one of the stages.  The spark `reduce` method seems like a better choice because in sample code above, all of the keys are the same.  In other words, we are only trying to compute standard deviation of temperatures for Boston in 2019.  If we were going to compute standard deviations across different cities or years, it seems like reduceByKey would be the correct option because we would need separate intermediate values on a per city / year basis.  But in this case, since all the keys are the same, one would think that the `reduce` method would be a more appropriate choice because the `reduce` method assumes all keys are the same. However, if you change `reduceByKey` to `reduce` you will introduce a runtime error.  Explain exactly why the runtime error happens in the designated cell below.  In order to receive full credit, you must identify the exact python / spark technical programming reason why changing from `reduceByKey` to `reduce` introduces a runtime error.  General answers that do not identify this exact technical reason will receive partial or no credit depending on the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag7BmiZzqD_b"
   },
   "source": [
    "#### Your explanation here:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8LuIXwvuU3q"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrh-KHJUuU3q"
   },
   "source": [
    "# Part 2: Spark Dataframes\n",
    "Part 2 uses the dataset from homework 1 to gain experience in using spark dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZYdEcvTuU3q"
   },
   "source": [
    "# Question 5: Gapminder Data From Homework 1 Revisited (10 pts)\n",
    "Load population, mortality, life expectancy, and fertility data into data frames named pop_df, mort_df, life_exp_df, and fert_df respectively.  Just like homework 1, rename the column with the country names as \"Country\".  Print the resulting shape of each dataframe along with a message indicating the specific dataframe name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5WHwjrSuU3q"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmxX7owEuU3r"
   },
   "outputs": [],
   "source": [
    "# grading cell do not modify\n",
    "pop_df_pd = pop_df.toPandas()\n",
    "display(pop_df_pd.head())\n",
    "print(pop_df_pd.shape)\n",
    "\n",
    "mort_df_pd = mort_df.toPandas()\n",
    "display(mort_df_pd.head())\n",
    "print(mort_df_pd.shape)\n",
    "\n",
    "life_exp_df_pd = life_exp_df.toPandas()\n",
    "display(life_exp_df_pd.head())\n",
    "print(life_exp_df_pd.shape)\n",
    "\n",
    "fert_df_pd = fert_df.toPandas()\n",
    "display(fert_df_pd)\n",
    "print(fert_df_pd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gyhwt8whuU3r"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LyhgreruU3r"
   },
   "source": [
    "# Melt\n",
    "The following cell introduces a pyspark implementation of melt found on [github](https://gist.github.com/korkridake/972e315e5ce094096e17c6ad1ef599fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1663265361864,
     "user": {
      "displayName": "Willard Williamson",
      "userId": "04507347240949254966"
     },
     "user_tz": 240
    },
    "id": "5p4wFFL7uU3r"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col, explode, lit, struct\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable \n",
    "\n",
    "def melt(\n",
    "        df: DataFrame, \n",
    "        id_vars: Iterable[str], value_vars: Iterable[str], \n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert :class:`DataFrame` from wide to long format.\n",
    "    Source: https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    # -------------------------------------------------------------------------------\n",
    "    _vars_and_vals = array(*(\n",
    "        struct(lit(c).alias(var_name), col(c).alias(value_name)) \n",
    "        for c in value_vars))\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    # Add to the DataFrame and explode\n",
    "    # -------------------------------------------------------------------------------\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-NGSxESuU3r"
   },
   "outputs": [],
   "source": [
    "# this cell tests the melt function above\n",
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame({'Country': {0: 'USA', 1: 'China', 2: 'India'},\n",
    "                   '2000': {0: 200, 1: 400, 2: 600},\n",
    "                   '2001': {0: 210, 1: 410, 2: 610},\n",
    "                   '2002': {0: 220, 1: 420, 2: 620}})\n",
    "\n",
    "print(\"pdf:\")\n",
    "display(pdf)\n",
    "\n",
    "print(\"pdf melt:\")\n",
    "display(pd.melt(pdf, id_vars=['Country'], var_name=\"Year\",value_name='Population'))\n",
    "\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "print(\"sdf melt\")\n",
    "display(melt(sdf, id_vars=['Country'], value_vars=['2000', '2001', '2002'], var_name='Year', value_name='Population').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_AV18tLuU3s"
   },
   "source": [
    "# Question 6 (10 pts): \n",
    "﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿Melt and then join all 4 dataframes together such that the country, year, population, mortality, life expectancy, and fertility columns are collected together in the same data frame just like homework 1. The join operation should not throw away any data. Name the new dataframe concat_df. Next, delete all rows where the data values (values other than country and year) are all NaN values. Only drop the row if ALL the data values are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kmZ1kWQuU3s"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3trzX_ouU3s"
   },
   "outputs": [],
   "source": [
    "# grading cell do not modify\n",
    "concat_pd_df = concat_df.toPandas()\n",
    "display(concat_pd_df.head())\n",
    "print(concat_pd_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve740YZOuU3s"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7gP0kbDuU3t"
   },
   "source": [
    "# Question 7 (10 pts):\n",
    "Examine if there was a long-life-in-a-small-family and short-life-in-a-large-family dichotomy in the data. Load continents.tsv into a new data frame named continents_df.  Create a new dataframe named concat1_df by joining continents_df with concat_df to create a new continents column in concat1_df.  Create a scatter plot of life expectancy versus fertiltiy for 1962 for Africa, Asia, Europe, and the Americas. Use color to denote continent and point size to denote population size. Do you see a dichotomy?  Explain your conclusions by supporting your conclusions with observations from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOHPocCCuU3t"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPxPt35CuU3t"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBQxtEEMuU3t"
   },
   "source": [
    "Your explanation here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUfJ127juU3t"
   },
   "outputs": [],
   "source": [
    "# grading cell do not modify\n",
    "concat1_df_pd = concat1_df.toPandas()\n",
    "display(concat1_df_pd.head())\n",
    "print(concat1_df_pd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQxbB4zwuU3u"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPwGgKCwuU3u"
   },
   "source": [
    "# Question 8 (10 pts):\n",
    "Learn about OECD and OPEC countries. OECD member countries can be found [here](https://www.oecd.org/about/document/list-oecd-member-countries.htm).  OPEC member countries can be found [here](https://www.opec.org/opec_web/en/about_us/25.htm).  Create a new dataframe named concat_df_oecd_opec.  Starting with the dataframe created in question 7, add a new column to concat_df_oecd_opec containing a logical vector that tells if a country is OECD and OPEC respectively. Create a new spark summary dataframe named totals_df that contains 1 row and 2 columns. Populate totals_df columns with the count of OPEC and OECD countries from concat_df_oecd_opec.  Name the columns in totals_df OPEC_total and OECD_total.  Make the same plot as in question 7 above, but this time use color to annotate the OECD countries and OPEC countries. For countries that are not part of these two organization annotate if they are from Africa, Asia, or the Americas.  Do you see a long-life-in-a-small-family and short-life-in-a-large-family dichotomy in the data.  Explain your conclusions by supporting your conclusions with observations from the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZcMCKjJuU3u"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60LGKP6ZuU3u"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOalo48HuU3v"
   },
   "source": [
    "Your explanation here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TfSU0ARuU3v"
   },
   "outputs": [],
   "source": [
    "# grading cell - do not modify\n",
    "concat_df_oecd_opec_pd = concat_df_oecd_opec.toPandas()\n",
    "display(concat_df_oecd_opec_pd.head())\n",
    "print(concat_df_oecd_opec_pd.shape)\n",
    "\n",
    "display(totals_df.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IL4kN3NYuU3v"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFHNpYlCz68C"
   },
   "source": [
    "# Extra Credit (5 points):\n",
    "Map reduce is really fun!  It's so much fun, we are going to extend the rdd class discussed in lecture and implement the reduce method as extra credit.  You will receive 5 extra credit points if you successfully implement the reduce method in the rdd class below.  Remember that rdd's are immutable so in order to get full credit, your implementation of must not change the internal data in the rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cex2OlkFz68J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define a few call back functions for test purposes\n",
    "def square(el):\n",
    "  return el * el\n",
    "\n",
    "def sum(el1, el2):\n",
    "  return el1 + el2\n",
    "\n",
    "def less_than_60(el):\n",
    "  if el < 60:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# define our own simple as possible rdd class\n",
    "class rdd:\n",
    "    def __init__(self, arg_array):\n",
    "        self.local_array = arg_array\n",
    "    \n",
    "    # define our own map function\n",
    "    def map(self, fun):\n",
    "        if len(self.local_array) < 1:\n",
    "            raise Exception(\"my_map: len(local_array) < 1\")\n",
    "        \n",
    "        # create a new numpy array to hold the result\n",
    "        # remember that rdd's are supposed to be immutable\n",
    "        mapped = np.empty_like(self.local_array)\n",
    "        \n",
    "        # for each element in the local array\n",
    "        for i, el in enumerate(self.local_array):\n",
    "            # execute the user defined function on this element\n",
    "            mapped[i] = fun(self.local_array[i])\n",
    "        \n",
    "        # This is the key to how the data is passed.  The return value is another rdd object!\n",
    "        # The fact that we are returning a new rdd is the key to how chaining works\n",
    "        # create a new rdd using the array created by applying the user defined function\n",
    "        return rdd(mapped)\n",
    "    \n",
    "    def filter(self, fun):\n",
    "        # create a temporary list to store the filtered results\n",
    "        transformed = []\n",
    "\n",
    "        for el in self.local_array:\n",
    "            if fun(el):\n",
    "                transformed.append(el)\n",
    "                \n",
    "        return rdd(np.array(transformed))\n",
    "    \n",
    "    # define our own reduce function\n",
    "    def reduce(self, fun):\n",
    "        # your code here\n",
    "        pass\n",
    "    \n",
    "    def collect(self):\n",
    "        return self.local_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQyMZforaT_"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C42qQHZz68K"
   },
   "outputs": [],
   "source": [
    "# Grading cell do not modify\n",
    "\n",
    "# create a my_rdd variable with a short list of numbers\n",
    "my_rdd = rdd(np.array([2,4,6,8, 10]))\n",
    "print(my_rdd.map(square).filter(less_than_60).reduce(sum).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjX1sBKIrgdW"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
