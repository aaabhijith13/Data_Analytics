{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNfPaiAQsgBf"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmrHfEEjfShT"
   },
   "source": [
    "# Question 0 (-2 If not answered)\n",
    "Please provide the following the data so we can verify your github information and ensure accurate grading:\n",
    "- Your Name:Abhijith Anil Vamadev\n",
    "- Your SU ID: 495204994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svGJOJtifShH"
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professors: \n",
    "  - Willard Williamson <wewillia@syr.edu>\n",
    "  - Emory Creel <emcreel@g.syr.edu>\n",
    "- Faculty Assistants: \n",
    "  - Warren Justin Fernandes <wjfernan@syr.edu>\n",
    "  - Ruchita Hiteshkumar Harsora <\trharsora@g.syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
    "- Google Colab is the official class runtime environment so you should test your code on Colab before submission.\n",
    "- Do not modify cells marked as grading cells or marked as do not modify.\n",
    "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
    "`Runtime `$\\rightarrow$ Factory reset runtime followed by Runtime $\\rightarrow$ Run All.  All runtime errors will result in a minimum penalty of half off.\n",
    "- All plots shall include descriptive title and axis labels.  Plot legends shall be included where possible.  Unless stated otherwise, plots can be made using any Python plotting package.\n",
    "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells.\n",
    "- Don't add or remove files from your git repo.\n",
    "- Do not change file names in your repo.  This also means don't change the title of the ipython notebook.\n",
    "- You are free to add additional code cells around the cells marked `your code here`.\n",
    "- import * is not allowed because it is considered a very bad coding practice and in some cases can result in a significant delay (which slows down the grading process) in loading imports.  For example, the statement `from sympy import *` is not allowed.  You must import the specific packages that you need. \n",
    "- The graders reserve the right to deduct points for subjective things we see with your code.  For example, if we ask you to create a pandas data frame to display values from an investigation and you hard code the values, we will take points off for that.  This is only one of many different things we could find in reviewing your code.  In general, write your code like you are submitting it for a code peer review in industry.  \n",
    "- Level of effort is part of our subjective grading.  For example, in cases where we ask for a more open ended investigation, some students put in significant effort and some students do the minimum possible to meet requirements.  In these cases, we may take points off for students who did not put in much effort as compared to students who put in a lot of effort.  We feel that the students who did a better job deserve a better grade.  We reserve the right to invoke level of effort grading at any time.\n",
    "- Your notebook must run from start to finish without requiring manual input by the graders.  For example, do not mount your personal Google drive in your notebook as this will require graders to perform manual steps.  In short, your notebook should run from start to finish with no runtime errors and no need for graders to perform any manual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BevKDzftfrm-"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Do not change or modify this cell\n",
    "# Need to install pyspark\n",
    "# if pyspark is already installed, will print a message indicating pyspark already installed\n",
    "pip install pyspark >& /dev/null \n",
    "\n",
    "# Download the data files from github\n",
    "# If the data file does not exist in the colab environment\n",
    "data_file_1=US_Airline_Tweets.csv\n",
    "\n",
    "if [[ ! -f ./${data_file_1} ]]; then \n",
    "   # download the data file from github and save it in this colab environment instance\n",
    "   wget https://raw.githubusercontent.com/wewilli1/ist718_data/master/${data_file_1} >& /dev/null \n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVxonwEusgBt"
   },
   "source": [
    "# Sentiment Analysis\n",
    "In this assignment, you will use a twitter US airline dataset to perform sentiment analysis.  Specifically, you will use twitter data to predict the sentiment of tweets related to peoples experience with an airline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1tYn9Sc_v6-C"
   },
   "outputs": [],
   "source": [
    "# Grading cell\n",
    "# The purpose of the following boolean is to enable or disable grid search (see question 6a).  \n",
    "# During grading we want to turn grid search off.  \n",
    "# You should test your code with grid search set to False before submitting.\n",
    "# Your notebook should run in its entirety without crashing when enable_grid is\n",
    "# set to False before submitting.\n",
    "enable_grid = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZzjDc-QN6E8"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlXXgoe3sgBu"
   },
   "source": [
    "# Qustion 1: (10 pts)\n",
    "Read US_Airline_Tweets.csv into a spark dataframe named `tweets_df`.  Drop all columns except airline_sentiment, airline, and text.  Drop rows in which the airline_sentiment column is labeled with a neutral sentiment.  Drop rows which contain NA / Null values in any column. Transform the airline_sentiment column such that a negative sentiment is equal to 0 and a positive sentiment is equal to 1.  This dataset has a lot more negative than positive tweets.  Balance the dataset such that the percentage of negative and positive tweets is roughly 50% each.  Your solution must **randomly sample** the dataset **without replacement** to perform balancing.  Determine and print the resulting percentage of positive and negative tweets in the dataframe such that it's easy for the graders to find and interpret your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8Wl62qN9sgBu"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col,struct,when\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import StringType, StructType, IntegerType, StructField, BooleanType, DoubleType\n",
    "from pyspark.sql.functions import col, sumDistinct, udf\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "#start spark\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "tweets_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"US_Airline_Tweets.csv\") #reading all files\n",
    "tweets_df = tweets_df.drop(\"tweet_id\",\"airline_sentiment_confidence\",\n",
    "                           \"negativereason\",\"negativereason_confidence\", \n",
    "                           \"name\", \"airline_sentiment_gold\",\n",
    "                           \"negativereason_gold\", \"retweet_count\", \n",
    "                           \"tweet_coord\", \"tweet_created\", \"tweet_location\", \n",
    "                           \"user_timezone\")\n",
    "tweets_df = tweets_df.filter(tweets_df.airline_sentiment != 'neutral')\n",
    "tweets_df = tweets_df.na.drop(\"any\")\n",
    "def cre(inp):\n",
    "  if(inp == 'positive'): # if the value of charges is less than or equal to the median returns 0 else 1\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "\n",
    "udf_func = udf(cre,IntegerType()) #creating user defined function\n",
    "tweets_df = tweets_df.withColumn('airline_sentiment',udf_func(tweets_df.airline_sentiment).cast('int')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMQ2GdZcG06d",
    "outputId": "5249a3ea-03ed-4758-df92-a6f85cae3674"
   },
   "outputs": [],
   "source": [
    "print(tweets_df.filter(tweets_df.airline_sentiment == 1).count())\n",
    "#minor_df = tweets_df.filter(tweets_df.airline_sentiment == 1)\n",
    "#major_df = tweets_df.filter(tweets_df.airline_sentiment == 0)\n",
    "#ratio = major_df.count()/minor_df.count()\n",
    "#print(\"Before sampling, Positive vs Negative sentiment: {} vs {}\".format(minor_df.count(), major_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v0UDFSdLPy2"
   },
   "outputs": [],
   "source": [
    "tweets_df_sample = tweets_df.sampleBy('airline_sentiment',{0: 1/ratio, 1: 1})\n",
    "mi_df = tweets_df.filter(tweets_df.airline_sentiment == 1)\n",
    "ma_df = tweets_df.filter(tweets_df.airline_sentiment == 0)\n",
    "tweets_df = tweets_df_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usF0nY037hcW",
    "outputId": "72c3489f-d750-4987-cbda-d8031e0b6d25"
   },
   "outputs": [],
   "source": [
    "print(\"After Sampling, Positive vs Negative sentiment: {} vs {}\".format(tweets_df_sample.filter(tweets_df_sample.airline_sentiment == 1).count(),tweets_df_sample.filter(tweets_df_sample.airline_sentiment == 0).count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "2G8lflJ6sgBv",
    "outputId": "dea61db0-4ccf-46b6-f07e-bd0e42d5a36b"
   },
   "outputs": [],
   "source": [
    "# grading cell do not modify\n",
    "tweets_pd = tweets_df.toPandas()\n",
    "display(tweets_pd.head())\n",
    "print(tweets_pd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8SRU7ZPsgBw"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmTNjNPtsgBw"
   },
   "source": [
    "# Question 2: (10 pts)\n",
    "Pre-process the data by creating a pipeline named `tweets_pre_proc_pipe`. Your pipeline should tokenize, remove stop words, and do a TF-IDF transformation.  Fit and execute your pipeline, and create a new dataframe named `tweets_pre_proc_df`.  Print the shape of the resulting TF-IDF data such that it's easy for the graders to find and understand as num rows x num words. Based on the shape of the TF-IDF data, would you expect a logistic regression model to overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEb3xvK8sgBx",
    "outputId": "a82e5f09-bc7b-4fde-d649-0db763423f5d"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "from pyspark.ml import Pipeline\n",
    "import requests\n",
    "from pyspark.ml.feature import RegexTokenizer, IDF,CountVectorizer, StopWordsRemover, HashingTF, HashingTF\n",
    "\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"filtered\")\\\n",
    "  .setOutputCol(\"tf\")\n",
    "\n",
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"idf\")\n",
    "\n",
    "tweets_pre_proc_pipe =Pipeline(stages=[tokenizer, sw_filter, cv, idf])\n",
    "tweets_fitted = tweets_pre_proc_pipe.fit(tweets_df)\n",
    "tweets_pre_proc_df = tweets_fitted.transform(tweets_df)\n",
    "print(\"Shape: {}, {}\".format(tweets_pre_proc_df.count(), len(tweets_fitted.stages[-2].vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sR-RcabSsgBy",
    "outputId": "6f7af1d4-ef6b-4041-fbec-9e60098544ce"
   },
   "outputs": [],
   "source": [
    "# grading cell do not modify\n",
    "tweets_pre_proc_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqOueDw9N8sO"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4BtpaqhsgB0"
   },
   "source": [
    "Your explanation here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzSNnallsgB0"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HuWaSIgsgB1"
   },
   "source": [
    "# Question 3: (10 pts)\n",
    "Since IDF considers a word's frequency across all documents in a corpus, you can use IDF as a form of inference.  Examine the documentation for the spark ML object that you used to create TF-IDF scores and learn how to extract the IDF scores for words in the corpus.  The idf object in your pipeline has a `values` attribute and a `tolist()` method which can be used to extract IDF values.  Create a pandas dataframe containing the 5 most important IDF scores named `most_imp_idf`.  Create another pandas dataframe containing the 5 least important IDF scores named `least_imp_idf`.  Each dataframe shall have 2 columns named `word` and `idf_score`.  Explain in words your interpretation of what the IDF scores mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N41HrFGgsgB1"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "#dir(tweets_fitted.stages[-1])\n",
    "#tweets_fitted.stages[-1].docFreq)\n",
    "list_idf = tweets_fitted.stages[-1].idf.tolist()\n",
    "\n",
    "low_value = pd.Series(list_idf[:5], name = \"idf_score\")\n",
    "high_value = pd.Series(list_idf[-1:-6:-1], name = \"idf_score\")\n",
    "\n",
    "vocab_list =  tweets_fitted.stages[-2].vocabulary\n",
    "high_vocab =  pd.Series(vocab_list[:5], name = \"Word\")\n",
    "low_vocab = pd.Series(vocab_list[-1:-6:-1], name = \"Word\")\n",
    "\n",
    "most_imp_idf =pd.concat([high_vocab, high_value], axis=1)\n",
    "least_imp_idf = pd.concat([low_vocab, low_value], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "uumED0kYsgB2",
    "outputId": "1bda5c3e-0f27-4f08-be1d-9ab3d64d5598"
   },
   "outputs": [],
   "source": [
    "# grading cell do not modify\n",
    "display(most_imp_idf)\n",
    "display(least_imp_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPzl6yqmK2-s"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwZgc3jGsgB2"
   },
   "source": [
    "Your explanation here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNa3LZeYsgB2"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KH7N8VBzsgB2"
   },
   "source": [
    "# Question 4: (10 pts)\n",
    "Create a new recursive pipeline named `lr_pipe` which encapsulates `tweets_pre_proc_pipe` and adds a logistic regression model and any needed logistic regression support objects.  Use default logistic regression hyper parameters.  Fit lr_pipe using `tweets_df`.  Score the model using ROC AUC.  Report the resulting AUC such that it is easy for graders to find and interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFv-EWXNsgB3"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "training_df, validation_df, testing_df = tweets_df.randomSplit([0.6, 0.3, 0.1], seed=0)\n",
    "[training_df.count(), validation_df.count(), testing_df.count()]\n",
    "\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('airline_sentiment').\\\n",
    "    setFeaturesCol('idf').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.)\n",
    "lr_pipe = Pipeline(stages=[tweets_pre_proc_pipe, lr]).fit(training_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6u5o6QGDATe"
   },
   "outputs": [],
   "source": [
    "# see https://stackoverflow.com/questions/52847408/pyspark-extract-roc-curve\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "# Scala version implements .roc() and .pr()\n",
    "# Python: https://spark.apache.org/docs/latest/api/python/_modules/pyspark/mllib/common.html\n",
    "# Scala: https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/evaluation/BinaryClassificationMetrics.html\n",
    "class CurveMetrics(BinaryClassificationMetrics):\n",
    "    def __init__(self, *args):\n",
    "        super(CurveMetrics, self).__init__(*args)\n",
    "\n",
    "    def _to_list(self, rdd):\n",
    "        points = []\n",
    "        # Note this collect could be inefficient for large datasets \n",
    "        # considering there may be one probability per datapoint (at most)\n",
    "        # The Scala version takes a numBins parameter, \n",
    "        # but it doesn't seem possible to pass this from Python to Java\n",
    "        for row in rdd.collect():\n",
    "            # Results are returned as type scala.Tuple2, \n",
    "            # which doesn't appear to have a py4j mapping\n",
    "            points += [(float(row._1()), float(row._2()))]\n",
    "        return points\n",
    "\n",
    "    def get_curve(self, method):\n",
    "        rdd = getattr(self._java_model, method)().toJavaRDD()\n",
    "        return self._to_list(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "yq2RQKlXDD8F",
    "outputId": "404ea7c8-e321-478a-eb01-ea586f55d511"
   },
   "outputs": [],
   "source": [
    "# Create a Pipeline estimator and fit on train DF, predict on test DF\n",
    "\n",
    "predictions = lr_pipe.transform(validation_df)\n",
    "# Returns as a list (false positive rate, true positive rate)\n",
    "preds = predictions.select('airline_sentiment','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['airline_sentiment'])))\n",
    "points = CurveMetrics(preds).get_curve('roc')\n",
    "\n",
    "plt.figure()\n",
    "x_val = [x[0] for x in points]\n",
    "y_val = [x[1] for x in points]\n",
    "plt.title('IMDB Sentiment Analysis ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.plot(x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2HexGEIDbFx",
    "outputId": "852932fc-db83-40b4-94a0-81cfec3d7aa7"
   },
   "outputs": [],
   "source": [
    "lr_pipe.transform(validation_df).select(fn.expr('float(prediction = airline_sentiment)').alias('correct')).\\\n",
    "    select(fn.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGU4OswusgB3"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTd-1FLSsgB3"
   },
   "source": [
    "# Question 5: (10 pts)\n",
    "Create 2 pandas dataframes named `lr_pipe_df_neg` and `lr_pipe_df_pos`which contain 2 colunms: `word` and `score`.  Load the 2 dataframes with the top 10 words and logistic regression coefficients that contribute the most to negative and positive sentiments respectively. Analyze the 2 dataframes and describe if the words make sense.  Do the words look like they are really negative and positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdBlW7IBsgB4"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "cv_words = pd.Series(lr_pipe.stages[-2].stages[-2].vocabulary, name = \"Word\")\n",
    "scores = pd.Series(lr_pipe.stages[-1].coefficients, name = \"Score\")\n",
    "words_score_df = pd.concat([cv_words, scores], axis=1)\n",
    "words_score_df = words_score_df.sort_values(by=['Score'], ascending = False)\n",
    "lr_pipe_df_neg = words_score_df.tail()\n",
    "lr_pipe_df_pos = words_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "v0Onlh5ksgB4",
    "outputId": "d94e1e94-1832-4359-9b09-b666b4ee57e4"
   },
   "outputs": [],
   "source": [
    "# grading cell - do not modify\n",
    "display(lr_pipe_df_neg)\n",
    "display(lr_pipe_df_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qV5rzFKLmjS"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdzhAZm7sgB4"
   },
   "source": [
    "Your explanation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebiGTaz_sgB5"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voK5KKVusgB5"
   },
   "source": [
    "# Question 6a: (5 pts)\n",
    "The goal of this question is to try to improve the score from question 4 using an elastic net regularization grid search on a new pipeline named `lr_pipe_1`. lr_pipe_1 is the same as lr_pipe above but we would like you to create a new pipe for grading purposes only.  I'm not sure if it's possible to increase the score or not.  You will be graded on level of effort to increase the score in relation to other students in the class.  All of your grid search code should be inside the `if enable_grid` statement in the cell below.  The enable_grid boolean is set to true in a grading cell above.  If any of the grid search code executes outside of the if statement, you will not get full credit for the question.  We want the ability to turn off the grid search during grading.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLruVEZdsgB5"
   },
   "outputs": [],
   "source": [
    "# your grid search (and only your grid search) code here\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "if enable_grid:\n",
    "    # your grid search code here\n",
    "    en_lr = LogisticRegression().\\\n",
    "        setLabelCol('airline_sentiment').\\\n",
    "        setFeaturesCol('idf')\n",
    "    lr_pipe_1 = Pipeline(stages=[tweets_pre_proc_pipe, en_lr])\n",
    "    lr_pipe_1_fitted = lr_pipe_1.fit(training_df)\n",
    "    #lr_pipe_1.transform(validation_df).select(fn.avg(fn.expr('float(prediction = airline_sentiment)'))).show()\n",
    "    grid = ParamGridBuilder().\\\n",
    "    addGrid(en_lr.regParam, [0., 0.01, 0.02, 0.004]).\\\n",
    "    addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4, 0.8]).\\\n",
    "    addGrid(en_lr.maxIter, [0, 10, 50, 100, 300]).\\\n",
    "    addGrid(en_lr.threshold, [0, 0.5, 0.1, ]).\\\n",
    "    build()\n",
    "    grid\n",
    "    all_models = []\n",
    "    for j in range(len(grid)):\n",
    "        print(\"Fitting model {}\".format(j+1))\n",
    "        model = lr_pipe_1.fit(training_df, grid[j])\n",
    "        all_models.append(model)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "7u-TpV51MmBD",
    "outputId": "626fa795-e300-413c-e674-9f695ff4b530"
   },
   "outputs": [],
   "source": [
    "# estimate the accuracy of each of them:\n",
    "import numpy as np\n",
    "accuracies = [m.\\\n",
    "    transform(validation_df).\\\n",
    "    select(fn.avg(fn.expr('float(airline_sentiment = prediction)')).alias('accuracy')).\\\n",
    "    first().\\\n",
    "    accuracy for m in all_models]\n",
    "best_model_idx = np.argmax(accuracies)\n",
    "print(\"best model index =\", best_model_idx) \n",
    "grid[best_model_idx]\n",
    "best_model = all_models[best_model_idx]\n",
    "print(accuracies[best_model_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIlUTXXjS19T",
    "outputId": "c8ba3510-c261-4279-d188-0524ec831745"
   },
   "outputs": [],
   "source": [
    "best_model_idx = np.argmax(accuracies)\n",
    "print(\"best model index =\", best_model_idx) \n",
    "grid[best_model_idx]\n",
    "best_model = all_models[best_model_idx]\n",
    "print(accuracies[best_model_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "0LWr2bDLNEZg",
    "outputId": "c140af51-b04c-4276-f111-ec8764e1fdba"
   },
   "outputs": [],
   "source": [
    "# estimate generalization performance\n",
    "best_model.\\\n",
    "    transform(testing_df).\\\n",
    "    select(fn.avg(fn.expr('float(airline_sentiment = prediction)')).alias('accuracy')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScGvacYlsgB5"
   },
   "source": [
    "##### Grading feedback cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS9pI45ysgB5"
   },
   "source": [
    "# Question 6b (5 pts)\n",
    "Build a new pipeline named `lr_pipe_2` which uses the optimized model parameters from the grid search in question 6a above (the best model).  Create 2 variables named alpha and lambda and assign to them the best alpha and lambda produced by the grid search by hard coding the values. Fit and transform lr_pipe_2.  Compare AUC scores between lr_pipe_2 with lr_pipe in question 4.  Create a pandas dataframe named `comapre_1_df` which encapsulates the comparison data.  comapre_1_df Shall have 2 columns: `model_name` and `auc_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxbOKM6UsgB6",
    "outputId": "4db62915-3926-4810-e9e7-496279e4e150"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_sentiment: int, airline: string, text: string, words: array<string>, filtered: array<string>, tf: vector, idf: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your optimized model code here\n",
    "# example\n",
    "# alpha = 0.1\n",
    "# lambda = 0.1\n",
    "elasticNetParam = best_model.stages[-1].getElasticNetParam()\n",
    "regParam =best_model.stages[-1]. getRegParam()\n",
    "maxIter = best_model.stages[-1].getMaxIter()\n",
    "threshold = best_model.stages[-1].getThreshold()\n",
    "\n",
    "lambda_par = 0.01\n",
    "alpha_par = 0.4\n",
    "en_lr_2 = LogisticRegression().\\\n",
    "        setLabelCol('airline_sentiment').\\\n",
    "        setFeaturesCol('idf').\\\n",
    "        setRegParam(lambda_par).\\\n",
    "        setMaxIter(maxIter).\\\n",
    "        setElasticNetParam(alpha_par).\\\n",
    "        setThreshold(threshold)\n",
    "# lr_pipe_2 code here which uses the best alpha and lambda\n",
    "lr_pipe_2 = Pipeline(stages=[tweets_pre_proc_pipe, en_lr_2])\n",
    "lr_pipe_2_fitted = lr_pipe_2.fit(training_df)\n",
    "lr_pipe_2_fitted.transform(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "VYZA46thsgB6",
    "outputId": "55f873cd-9f77-4bc9-c661-f29826402d29"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8272cd4f2924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# grading cell - do not modify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomapre_1_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'comapre_1_df' is not defined"
     ]
    }
   ],
   "source": [
    "# grading cell - do not modify\n",
    "display(comapre_1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXdATaoUsgB6"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFs4wt5EsgB6"
   },
   "source": [
    "# Question 7 (10 pts)\n",
    "Perform inference on lr_pipe_2.  Write code to report how many words were eliminated from the best model in question 6b above (if any) as compared to the model in question 4 above.  Make sure your output is easy for the graders to find and interpret.\n",
    "\n",
    "Describe in words how feature selection is performed using elastic net regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ir0jt_eCsgB6"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gizxpUBRsgB7"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDF3MSNGMSut"
   },
   "source": [
    "Your explanation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq7sCMG9MT5v"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g8OEf4UsgB7"
   },
   "source": [
    "# Question 8 (10 pts)\n",
    "Perform the same inference analysis that you did in question 5 but name the data frames `lr_pipe_df_neg_1` and `lr_pipe_df_pos_1`.  Compare the word importance results with the results in question 5.  Do the most positive and most negative words produced by using regularization better reflect positive and negative sentiment than the most positive and negative words produced by the model that did not use regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x_hxUyMsgB7"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdCz06VosgB7"
   },
   "outputs": [],
   "source": [
    "# grading cell - do not modify\n",
    "display(lr_pipe_df_neg_1)\n",
    "display(lr_pipe_df_pos_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzG2lc6ksgB7"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGjAkZENsgB8"
   },
   "source": [
    "Your explanation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFrYfkWfsgB8"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhf2hfVUsgB8"
   },
   "source": [
    "# Question 9 (10 pts)\n",
    "Precision recall plots are very similar to receiver operating characteristic (ROC) curves.  The high level steps for creating a precision recall curve are the same as the steps needed to create a ROC curve as outlined in lecture. Learn about [precision recall curves](https://en.wikipedia.org/wiki/Precision_and_recall).  Create a precision recall plot for the best model in question 6.  Describe what axes are the same / different between the precision recall curve and the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kys4SqgvsgB8"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUnE8Qb1Nxr3"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG7lJsd3sgB8"
   },
   "source": [
    "Your explanation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atfg5seiNxEI"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt76hKF8BRcW"
   },
   "source": [
    "# Question 10 (10 pts)\n",
    "Design and implement a method to rank the airlines in your dataset from best to worst.  Your solution can use model predictions or inference or both to perform this task. Implement your ranking algorithm in spark.  Create a spark dataframe named airline_rankings.  airline_rankings Shall have 3 columns: airline_name, num_reviews, and ranking.  Load the num_reviews column with the number of reviews associated with the airline.  Load the ranking column with your rank calculation result.  Sort airline rankings from best to worst (best at head, worst at tail).  \n",
    "\n",
    "Describe in words how your algorithm works in clear easy to understand language.  We will take points off for descriptions that are not clearly stated and easy to follow.  We don't expect to have to reverse engineer your code understand how your algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2th6hjcDEM6"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjZOXElJNzhJ"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6PiT5FEDFpA"
   },
   "source": [
    "Your algorithm description here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWG1DI5mDeSg"
   },
   "outputs": [],
   "source": [
    "# grading cell do not modify\n",
    "display(airline_rankings.toPandas().head())\n",
    "display(airline_rankings.toPandas().tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntyK4q19N1wL"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj2KN5cawifc"
   },
   "source": [
    "# Question 11 (0 pts)\n",
    "Make sure to set enable_grid to False in the grading cell above and run the notebook in its entirety before submitting to verify that there are no runtime erros.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rUpGwM4N3w7"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
